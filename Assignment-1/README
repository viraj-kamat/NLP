Implementation details :

Training the model and hyper-parameters :
The initial training of the model was done with default hyper-parameters, but were fine tuned to provide better performance.

The following are the configuration details of each model built:

================================================================================================================

Cross Entropy 1:
batch_size : 128
embedding_size : 128
learning_rate : 0.2
skip_window : 2
num_skips : 4
max_num_steps : 200001
accuracy : 33.3

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_cross_entropy_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   309
Number of Least Illustrative Guessed Incorrectly: 605
Accuracy of Least Illustrative Guesses:            33.8%
Number of Most Illustrative Guessed Correctly:    299
Number of Most Illustrative Guessed Incorrectly:  615
Accuracy of Most Illustrative Guesses:             32.7%
Overall Accuracy:                                  33.3%


Cross entropy 2:
batch_size : 256
embedding_size : 128
learning_rate : 1
skip_window : 2
num_skips : 4
max_num_steps : 300001
accuracy : 33.5

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_cross_entropy_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   310
Number of Least Illustrative Guessed Incorrectly: 604
Accuracy of Least Illustrative Guesses:            33.9%
Number of Most Illustrative Guessed Correctly:    302
Number of Most Illustrative Guessed Incorrectly:  612
Accuracy of Most Illustrative Guesses:             33.0%
Overall Accuracy:                                  33.5%


Cross entropy 3 : 
batch_size : 256
embedding_size : 128
learning_rate : 0.5
skip_window : 1
num_skips : 2
max_num_steps : 500001
accuracy : 32.7

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_cross_entropy_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   301
Number of Least Illustrative Guessed Incorrectly: 613
Accuracy of Least Illustrative Guesses:            32.9%
Number of Most Illustrative Guessed Correctly:    297
Number of Most Illustrative Guessed Incorrectly:  617
Accuracy of Most Illustrative Guesses:             32.5%
Overall Accuracy:                                  32.7%

Cross entropy 4 : 
batch_size : 256
embedding_size : 128
learning_rate : 0.8
skip_window : 1
num_skips : 2
max_num_steps : 300001
accuracy : 32.8

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_cross_entropy_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   303
Number of Least Illustrative Guessed Incorrectly: 611
Accuracy of Least Illustrative Guesses:            33.2%
Number of Most Illustrative Guessed Correctly:    296
Number of Most Illustrative Guessed Incorrectly:  618
Accuracy of Most Illustrative Guesses:             32.4%
Overall Accuracy:                                  32.8%

Cross entropy 5 :
batch_size : 256
embedding_size : 128
learning_rate : 1
skip_window : 2
num_skips : 4
max_num_steps : 300001
accuracy : 33.5

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_cross_entropy_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   310
Number of Least Illustrative Guessed Incorrectly: 604
Accuracy of Least Illustrative Guesses:            33.9%
Number of Most Illustrative Guessed Correctly:    302
Number of Most Illustrative Guessed Incorrectly:  612
Accuracy of Most Illustrative Guesses:             33.0%
Overall Accuracy:                                  33.5%

Cross Entropy 6 :
batch_size : 256
embedding_size : 128
learning_rate : 1
skip_window : 4
num_skips : 8
max_num_steps : 500001
accuracy : 33.4

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_cross_entropy_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   311
Number of Least Illustrative Guessed Incorrectly: 603
Accuracy of Least Illustrative Guesses:            34.0%
Number of Most Illustrative Guessed Correctly:    302
Number of Most Illustrative Guessed Incorrectly:  612
Accuracy of Most Illustrative Guesses:             33.0%
Overall Accuracy:                                  33.4%


Cross Entropy 7 (Best Model) :
batch_size : 256
embedding_size : 128
learning_rate : 1
skip_window : 8
num_skips : 16
max_num_steps : 500001
accuracy : 33.7

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_cross_entropy_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   313
Number of Least Illustrative Guessed Incorrectly: 601
Accuracy of Least Illustrative Guesses:            34.2%
Number of Most Illustrative Guessed Correctly:    303
Number of Most Illustrative Guessed Incorrectly:  611
Accuracy of Most Illustrative Guesses:             33.2%
Overall Accuracy:                                  33.7%



NCE 1 :
batch_size : 128
embedding_size : 128
learning_rate : 0.2
skip_window : 2
num_skips : 4
max_num_steps : 200001
accuracy : 33.9

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_nce_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   316
Number of Least Illustrative Guessed Incorrectly: 598
Accuracy of Least Illustrative Guesses:            34.6%
Number of Most Illustrative Guessed Correctly:    303
Number of Most Illustrative Guessed Incorrectly:  611
Accuracy of Most Illustrative Guesses:             33.2%
Overall Accuracy:                                  33.9%



NCE 2 :
batch_size : 256
embedding_size : 128
learning_rate : 1
skip_window : 2
num_skips : 4
max_num_steps : 300001
accuracy : 34.2

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_nce_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   317
Number of Least Illustrative Guessed Incorrectly: 597
Accuracy of Least Illustrative Guesses:            34.7%
Number of Most Illustrative Guessed Correctly:    308
Number of Most Illustrative Guessed Incorrectly:  606
Accuracy of Most Illustrative Guesses:             33.7%
Overall Accuracy:                                  34.2%


NCE 3 :
batch_size : 256
embedding_size : 128
learning_rate : 0.5
skip_window : 1
num_skips : 2
max_num_steps : 500001
accuracy : 33.2




NCE 4 :
batch_size : 256
embedding_size : 128
learning_rate : 0.8
skip_window : 1
num_skips : 2
max_num_steps : 300001
accuracy : 33.4

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_nce_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   307
Number of Least Illustrative Guessed Incorrectly: 607
Accuracy of Least Illustrative Guesses:            33.6%
Number of Most Illustrative Guessed Correctly:    303
Number of Most Illustrative Guessed Incorrectly:  611
Accuracy of Most Illustrative Guesses:             33.2%
Overall Accuracy:                                  33.4%

NCE 5 :
batch_size : 256
embedding_size : 128
learning_rate : 1
skip_window : 2
num_skips : 4
max_num_steps : 300001
accuracy : 33.8

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_nce_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   313
Number of Least Illustrative Guessed Incorrectly: 601
Accuracy of Least Illustrative Guesses:            34.2%
Number of Most Illustrative Guessed Correctly:    304
Number of Most Illustrative Guessed Incorrectly:  610
Accuracy of Most Illustrative Guesses:             33.3%
Overall Accuracy:                                  33.8%

NCE 6 :
batch_size : 256
embedding_size : 128
learning_rate : 1
skip_window : 4
num_skips : 8
max_num_steps : 500001
accuracy : 34

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_nce_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   322
Number of Least Illustrative Guessed Incorrectly: 592
Accuracy of Least Illustrative Guesses:            35.2%
Number of Most Illustrative Guessed Correctly:    300
Number of Most Illustrative Guessed Incorrectly:  614
Accuracy of Most Illustrative Guesses:             32.8%
Overall Accuracy:                                  34.0%

NCE 7 (Best Model) :
batch_size : 256
embedding_size : 128
learning_rate : 1
skip_window : 8
num_skips : 16
max_num_steps : 500001
accuracy : 34.2

Generated by:                                     score_maxdiff.pl
Mechanical Turk File:                             word_analogy_dev_mturk_answers.txt
Test File:                                        word_analogy_nce_predictions.txt
Number of MaxDiff Questions:                      914
Number of Least Illustrative Guessed Correctly:   321
Number of Least Illustrative Guessed Incorrectly: 593
Accuracy of Least Illustrative Guesses:            35.2%
Number of Most Illustrative Guessed Correctly:    302
Number of Most Illustrative Guessed Incorrectly:  612
Accuracy of Most Illustrative Guesses:             33.0%
Overall Accuracy:                                  34.2%

================================================================================================================

Implementation Details :

Generate batch function :
The generate batch function is governed by the batch_size, skip_window and num_skips paramters. Here for each index on the data_index the context words to be picked before and after were selected using simple array slice notation which was determined by the skip_windiw parameter. The num_skips parameter was then used to pick context words for this center word which determines how many elements need to be picked. Then the index for the center word was slided one at a time and the context words were picked iteratively until the batch_size limit was reached.

================================================================================================================

Cross Entropy loss function :
In this function I first computed the dot product of inputs and true_w (element wise multiplication), this made it possible to get the numerator of the loss function by mapping the product of true input words with context words. After that the log and the exponent of each element was taken which helped form the part A of the loss function.

For Part B I took a matrix multiplication of inputs and true_w which enabled to compute the dot product of each of the input word with every other context word. The resultant matrix was then reshaped and its exponent and log was taken. This helped form part B of the loss function.

Finally (Part B - Part A) was returned.

================================================================================================================

NCE loss function :
To implement the NCE loss function I follows a near similar approach to cross entropy, though the math behind it was different.


Before starting I gathered the weights for the samples and the labels, the unigram probibilities for the samples and labels, and finally the biases for the samples and the labels.

To achieve the first part I multiplied element-wise the inputs and the weight labels. I then added the biases for labels to this result. (1).
Then I multipled the number of samples with the unigram probibilities of the labels and took its log.(2)

I finally subtracted (1) and (2), took this result's sigmoid and then its log forming part 1 of the overall equation. (Part 1)


For the second part of the equation I did the matrix multiplication of the inputs and the weight of the samples and added the biases of these samples to the result.(3)
I then multiplied the number of samples with the unigram probibilities of the samples and took its log. I then subtracted (3) and (4) and took its sigmoid.(5) 

I then subtracted from 1 every element of (5) step wise, and took its log. (Part 2)

I then subtracted Part 1 and Part 2 returning the results of the loss function.

================================================================================================================

Word analogy : After the word_analogy_dev sample file was loaded in the analogy script, I first extracted the example pairs from each line and sanitized it.
I looked up for the word vectors for each of this word pair and found this vector difference for each of this pair.
I then found the mean difference for the example word vector pair.
The same process was repeated for choices word pairs but the mean of their differences was not computed - the spatial cosine difference of the choices word pair was computed with respect to the mean of the difference of the example word pair vectors. The one with the least cosine difference was the least illustrative pair and the one with the most cosine difference was the most illustrative pair.

================================================================================================================

Top 20 words : For each of the target words "First","American", "Would" I looked up their respective word embeddings (vectors) and the word vectors for every other element in the dictionary. For each of our target words I computed the spatial cosine difference with every word vector in the dictionary and sorted the result from the most relevant to the least relevant.

I then picked the top 20 elements from this list for each of our target words.


